"""**Lesion Detection with Deep Learning + Classification with Radiomics**"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/08_adenoma_detection_pipeline.ipynb.

# %% auto 0
__all__ = ['DATA', 'DETECTION_DATA_FOLDER', 'CLASSIFICATION_DATA_FOLDER', 'RESULTS_FOLDER', 'parser', 'args_to_dict',
           'create_results_dir', 'convert_paths', 'data_preparation', 'load_detection_model',
           'load_classification_models', 'run_lesion_detection', 'LesionClasses', 'create_category_id_column',
           'get_dataset_dataframe', 'load_image', 'NormalizationType', 'min_max_normalize', 'preprocess_image',
           'create_roi_mask', 'preproc_pipeline', 'extract_radiomic_features', 'clean_features_df', 'expand_roi',
           'preproc_and_features_extraction', 'features_selection', 'evaluate_classification_model',
           'run_radiomics_pipeline', 'run_pipeline']

# %% ../nbs/08_adenoma_detection_pipeline.ipynb 2
from pathlib import Path
import SimpleITK as sitk
from SimpleITK import Image
from enum import Enum
from radiomics import featureextractor
import os
import random
import argparse
from adrenal_lesion_det.det_evaluation import (
    read_json,
    write_json,
)
from detectron2.config import instantiate
from .dl_training import prepare_data, prepare_config_before_instance
from detectron2.utils.logger import setup_logger
from detectron2.checkpoint import DetectionCheckpointer
from datetime import datetime
from joblib import load
from detectron2.structures import Boxes, Instances
from detectron2.structures import pairwise_iou
import torch
import numpy as np
import cv2
from typing import Union
import pandas as pd
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import classification_report, roc_auc_score
from matplotlib import pyplot as plt
import seaborn as sns
# from sklearn.linear_model import LassoCV
# from sklearn.decomposition import PCA
# from xgboost import XGBClassifier

# %% ../nbs/08_adenoma_detection_pipeline.ipynb 3
def parser():
    parser = argparse.ArgumentParser(
        description="Starting the detection+classification pipeline",
    )
    # Mandatory arguments with specified `metavar` format
    parser.add_argument(
        "-det_model",
        "-dm",
        metavar="-dm",
        type=str,
        help="name of the detection model",
        required=True,
    )
    parser.add_argument(
        "-cla_model",
        "-cm",
        metavar="-cm",
        type=str,
        help="name of the classification model",
        required=True,
    )
    parser.add_argument(
        "-cla_best",
        "-cb",
        metavar="-cb",
        type=str,
        help="prefix of the best classification",
        required=True,
    )

    parser.add_argument(
        "-dataset_name",
        "-d",
        metavar="-d",
        type=str,
        help="name of the dataset: train, test or val",
        required=True,
    )
    parser.add_argument(
        "-iou_thres",
        "-it",
        metavar="-it",
        type=str,
        help="IoU threshold for detection",
        required=True,
    )
    parser.add_argument(
        "-checkpoint",
        "-ckpt",
        metavar="-ckpt",
        type=str,
        help="checkpoint to evaluate",
        required=True,
    )
    parser.add_argument(
        "-detection_score",
        metavar="-ds",
        type=float,
        help="minimum score threshold for evaluation",
        required=False,
    )
    parser.add_argument(
        "-iou",
        metavar="-iou",
        type=float,
        help="minimum IoU threshold for classification",
        required=False,
        default=0.0,  # real life scenario
    )

    # Optional arguments without `metavar`
    parser.add_argument(
        "-raw_image",
        action="store_true",
        help="whether to process raw images (optional, boolean)",
        default=None,
    )
    parser.add_argument(
        "-crop_to_content",
        action="store_true",
        help="whether to crop images to content (optional, boolean)",
        default=None,
    )
    parser.add_argument(
        "-filter_no_annotations",
        action="store_true",
        help="whether to filter images with no annotations (optional, boolean)",
        default=None,
    )
    parser.add_argument(
        "-final_shape",
        type=int,
        nargs=2,
        help="final shape of the images as [width, height] (optional)",
        default=None,
    )
    return parser.parse_args()


def args_to_dict(args):
    return vars(args)

# %% ../nbs/08_adenoma_detection_pipeline.ipynb 4
def create_results_dir(results_path, dataset_name, classification_type):
    key = f"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_{dataset_name}_{classification_type}"
    res_dir = results_path / key
    res_dir.mkdir(parents=True, exist_ok=True)
    return res_dir

# %% ../nbs/08_adenoma_detection_pipeline.ipynb 5
def convert_paths(data):
    if isinstance(data, dict):
        return {k: convert_paths(v) for k, v in data.items()}
    elif isinstance(data, list):
        return [convert_paths(i) for i in data]
    elif isinstance(data, Path):
        return str(data)
    else:
        return data
    
def data_preparation(
    detection_model_folder: Path,
    dataset_name: str,
    raw_image: bool = None,
    crop_to_content: bool = None,
    filter_no_annotations: bool = None,
    final_shape: list[int] = None,
    args_dict: dict = None,
    results_folder: Path = Path("results"),
    classification_type: str = "binary",
):
    assert detection_model_folder.exists()
    cfg = read_json(detection_model_folder / "config.json")
    this_results_dir = create_results_dir(results_folder, dataset_name, classification_type)
    write_json(convert_paths(args_dict), this_results_dir / "args.json")
    logger = setup_logger((this_results_dir / "log.txt").as_posix(), abbrev_name="")
    logger.info(f"Preparing dataset based on config from {detection_model_folder}")
    logger.info(f"Args: {args_dict}")
    logger.info(f"Results saved in {this_results_dir}")
    dataloader = prepare_data(
        logger=logger,
        sets=[dataset_name],
        num_workers=8,
        filter_no_annotations=filter_no_annotations,
        crop_to_content=(
            cfg["preprocessing"]["crop_to_content"]
            if crop_to_content is None
            else crop_to_content
        ),
        roi=cfg["general"]["roi"] if raw_image is None else raw_image,
        dataframe_path=cfg["general"]["dataframe"],
        raw_slices_names=cfg["general"]["raw_slices"],
        normalisation_type=cfg["preprocessing"]["normalisation"],
        enhancement_type=cfg["preprocessing"]["enhancement"],
        final_shape= cfg["preprocessing"]["final_shape"] if final_shape is None else final_shape,
        batch_size=cfg["training"]["imgs_per_batch"],
        sampler_repeat_threshold=cfg["preprocessing"].get("sampler_repeat_threshold"),
        sampler_seed=cfg["preprocessing"].get("sampler_seed"),
        slices_type=cfg["general"].get("slices_type"),
        # slices_type="with_contrast",
        classes=cfg["general"].get("classes"),
    ) 
    return dataloader[dataset_name], cfg, logger, this_results_dir


# %% ../nbs/08_adenoma_detection_pipeline.ipynb 6
def load_detection_model(
    logger,
    dataloader,
    res_dir,
    det_cfg,
    dataset_name,
    detection_score,
    nms_threshold,
    checkpoint,
):
    
    config = prepare_config_before_instance(
            logger=logger,
            results_path=res_dir,
            dataloaders=dataloader,
            eval_dataset=dataset_name,
            model_base_config=det_cfg["model"]["model_base_config"],
            trained=det_cfg["model"]["trained"],
            classes=["lesion"],
            test_score=det_cfg["model"]["test_score_thresh"] if detection_score is None else detection_score,
            test=True
        )
    config.model.test_nms_thresh = nms_threshold if nms_threshold else 0.6
    model = instantiate(config.model)
    model.to("cuda")
    DetectionCheckpointer(model).load(str(checkpoint))
    return model, config


def load_classification_models(
    classification_model_folder: Path, best_model_prefix: str, logger
):
    assert classification_model_folder.exists()
    lasso_model_name = classification_model_folder / f"{best_model_prefix}-lasso.joblib"
    logger.info(f"Loading Lasso model from {lasso_model_name}")
    pca_model_name = classification_model_folder / f"{best_model_prefix}-pca.joblib"
    logger.info(f"Loading PCA model from {pca_model_name}")
    classifier_name = classification_model_folder / f"{best_model_prefix}-model.joblib"
    logger.info(f"Loading classifier model from {classifier_name}")
    return load(lasso_model_name), load(pca_model_name), load(classifier_name)

# %% ../nbs/08_adenoma_detection_pipeline.ipynb 8
def run_lesion_detection(model, config, iou_threshold, logger):
    model.eval()
    logger.info("Running batch inference. Extracting Predicted Boxes.")
    # for feature extraction and classification model
    true_positives = {}
    true_negatives = {}
    false_negatives = {}
    false_positives = {}
    for batch in config.dataloader:
        with torch.no_grad():
            batch_outputs = model(batch)
            for i, data in enumerate(batch):
                pred_instances = batch_outputs[i]["instances"].to("cpu")
                ground_truth_boxes = Boxes(
                    np.array([anno["bbox"] for anno in data["annotations"]])
                )
                if len(ground_truth_boxes) == 0 and len(pred_instances) == 0:
                    true_negatives[Path(data["file_name"]).stem] = data
                elif len(ground_truth_boxes) > 0 and len(pred_instances) == 0:
                    false_negatives[Path(data["file_name"]).stem] = (
                        data  # no predicted boxes but gt exists
                    )
                elif len(ground_truth_boxes) == 0 and len(pred_instances) > 0:
                    best_score = pred_instances.scores.max()
                    pred_instances = pred_instances[pred_instances.scores == best_score]
                    false_positives[Path(data["file_name"]).stem] = data
                    false_positives[Path(data["file_name"]).stem]["instances"] = (
                        pred_instances
                    )
                else:
                    best_score = pred_instances.scores.max()
                    pred_instances = pred_instances[pred_instances.scores == best_score]
                    # we have predictions and ground truth
                    iou_matrix = pairwise_iou(pred_instances.pred_boxes, ground_truth_boxes)
                    max_iou = iou_matrix.max().item()  # Get the maximum IoU value as a scalar
                    if (
                        max_iou
                        >= iou_threshold
                    ):
                        true_positives[Path(data["file_name"]).stem] = data
                        true_positives[Path(data["file_name"]).stem]["instances"] = (
                            pred_instances
                        )
                    else:
                        false_positives[Path(data["file_name"]).stem] = data
                        false_positives[Path(data["file_name"]).stem]["instances"] = (
                            pred_instances
                        )

            # torch.cuda.empty_cache()
    logger.info("Finished Detectron2 batch inference.")
    return (
        true_positives,
        true_negatives,
        false_positives,
        false_negatives,
    )

# %% ../nbs/08_adenoma_detection_pipeline.ipynb 9
class LesionClasses(Enum):
    NO_LESION = 2
    ADENOMA = 1
    NOT_ADENOMA = 0

    @classmethod
    def get_classes(cls, multiclass):
        # Exclude NO_LESION
        if not multiclass:
            filtered_classes = [cls for cls in LesionClasses if cls != LesionClasses.NO_LESION]
        else:
            filtered_classes = [cls for cls in LesionClasses]
        # Sort to ensure the class with value 0 is first
        sorted_classes = sorted(filtered_classes, key=lambda x: (x.value != 0, x.value))
        return [cls.name for cls in sorted_classes]


# %% ../nbs/08_adenoma_detection_pipeline.ipynb 10
def create_category_id_column(df: pd.DataFrame, multiclass: bool):
    aux = {
        None: LesionClasses.NO_LESION.value,
        "adenoma": LesionClasses.ADENOMA.value,
        "not_adenoma": LesionClasses.NOT_ADENOMA.value,
    }
    if not multiclass:
        aux[None] = LesionClasses.NOT_ADENOMA.value
        
    df["CategoryID"] = df["Label"].map(aux)
    return df


def get_dataset_dataframe(det_config, dataset_name, multiclass):
    aux = Path(det_config["general"]["dataframe"])
    data = pd.read_feather(
        Path(os.getenv("PHD_DATA")) / f"{aux.parent}/data_splitted/{dataset_name}_{aux.stem}.feather"
    )
    try:
        data.drop(columns=["Unnamed: 0", "index"], inplace=True)
    except Exception:
        pass
    return create_category_id_column(data, multiclass)

# %% ../nbs/08_adenoma_detection_pipeline.ipynb 11
# Radiomics Pipeline Methods


def load_image(path: str) -> Image:
    return sitk.ReadImage(path)


class NormalizationType(Enum):
    MINMAX = 2
    NONE = 1


def min_max_normalize(image: Image):
    array = sitk.GetArrayFromImage(image)
    min_val = np.min(array)
    max_val = np.max(array)
    normalized_array = (array - min_val) / (max_val - min_val)
    try:
        normalized_array = normalized_array[:, :, 0]
    except IndexError:
        pass 
            
    normalized_image = sitk.GetImageFromArray(normalized_array)
    normalized_image.CopyInformation(image)
    return normalized_image


def preprocess_image(
    image: Image, method: NormalizationType = NormalizationType.MINMAX
):
    if method == NormalizationType.MINMAX:
        return min_max_normalize(image)
    elif method == NormalizationType.NONE:
        return image
    else:
        raise ValueError(
            f"Normalization method not recognized. Use {list(NormalizationType.__members__.items())}."
        )


def create_roi_mask(image: Image, x1: int, y1: int, x2: int, y2: int):
    img_array = sitk.GetArrayFromImage(image)
    height, width = img_array.shape[:2]
    x1, x2 = max(0, x1), min(width, x2)
    y1, y2 = max(0, y1), min(height, y2)
    mask_array = np.zeros((height, width), dtype=np.uint8)
    mask_array[y1:y2, x1:x2] = 1
    mask = sitk.GetImageFromArray(mask_array)
    mask.CopyInformation(image)
    return mask


def preproc_pipeline(
    filepath: str,
    roi: tuple,
    method: NormalizationType = NormalizationType.MINMAX,
):
    image = load_image(filepath)
    mask = create_roi_mask(image, *roi)
    preproc_image = preprocess_image(image, method=method)
    return mask, preproc_image


def extract_radiomic_features(image, mask):
    extractor = featureextractor.RadiomicsFeatureExtractor()
    features = extractor.execute(image, mask)
    return features


def clean_features_df(features_df: pd.DataFrame):
    cols_to_drop = features_df.filter(like="Versions").columns
    features_df.drop(cols_to_drop, axis=1, inplace=True)
    cols_to_drop = features_df.filter(like="diagnostics").columns
    features_df.drop(cols_to_drop, axis=1, inplace=True)
    return features_df


def expand_roi(roi, factor, image_size):
    # Extract the coordinates from the ROI
    x_min, y_min, x_max, y_max = roi

    # Calculate width and height
    width = x_max - x_min
    height = y_max - y_min
    x_center = x_min + width / 2
    y_center = y_min + height / 2

    # Expand width and height by the factor
    new_width = width * factor
    new_height = height * factor

    # Calculate new corners
    new_x_min = x_center - new_width / 2
    new_y_min = y_center - new_height / 2
    new_x_max = x_center + new_width / 2
    new_y_max = y_center + new_height / 2

    # Clip the new coordinates to the image boundaries
    new_x_min = max(0, min(new_x_min, image_size[1]))  # Clamped to [0, image_width]
    new_y_min = max(0, min(new_y_min, image_size[0]))  # Clamped to [0, image_height]
    new_x_max = max(0, min(new_x_max, image_size[1]))
    new_y_max = max(0, min(new_y_max, image_size[0]))

    # Return the expanded ROI
    return [
        int(round(new_x_min)),
        int(round(new_y_min)),
        int(round(new_x_max)),
        int(round(new_y_max)),
    ]


def preproc_and_features_extraction(
    detection_results, dataframe, min_area, logger, box_expansion, multiclass: bool
) -> Union[list, pd.DataFrame]:
    logger.info("Preprocessing and extracting features.")
    ground_truth_cat = []
    features_list = []
    filenames = []
    excluded_by_area = []
    for filename in detection_results:
        detection = detection_results[filename]
        instances: Instances = detection["instances"]
        if len(instances) == 1:
            if instances.pred_boxes.area() > min_area:

                filenames.append(filename)
                try:
                    ground_truth_cat.append(
                        dataframe[dataframe.Filename == filename].CategoryID.values[0]
                    )
                except IndexError:
                    no_lesion_class = (
                        LesionClasses.NO_LESION.value
                        if multiclass
                        else LesionClasses.NOT_ADENOMA.value
                    )
                    ground_truth_cat.append(no_lesion_class)

                roi = [int(round(i, 0)) for i in instances.pred_boxes.tensor[0].numpy()]
                roi = expand_roi(roi, box_expansion, instances.image_size)
                mask, preproc_image = preproc_pipeline(
                    filepath=detection["file_name"],
                    roi=roi,
                    method=NormalizationType.MINMAX,
                )
                features = extract_radiomic_features(preproc_image, mask)
                features_list.append(features)
            else:
                excluded_by_area.append(filename)
                logger.info(
                    f"Skipping {detection['file_name']} because area is too small."
                )
        else:
            raise NotImplementedError
    return (
        ground_truth_cat,
        clean_features_df(pd.DataFrame(features_list)),
        filenames,
        excluded_by_area,
    )


def features_selection(features_df, lasso_model, pca_model, logger):
    logger.info("Selecting features.")
    selected_features = features_df.iloc[:, lasso_model.coef_ != 0].values
    return pca_model.transform(selected_features)


def evaluate_classification_model(
    model,
    X_test,
    y_test,
    confusion: bool = True,
    output_dict: bool = True,
    multiclass: bool = False,
):

    y_pred = model.predict(X_test)
    res = None
    roc_auc = None
    try:
        if multiclass:
            # ovr: Suitable when you are more interested in the overall ability of the classifier to distinguish between one class and the rest.
            roc_auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class="ovr")
        else:
            roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])
    except ValueError:
        pass
    if output_dict:
        res = classification_report(y_test, y_pred, output_dict=True)
        res["roc_auc"] = roc_auc
    if confusion:
        conf_matrix_data = confusion_matrix(y_test, y_pred)
    return y_pred, res, conf_matrix_data


def run_radiomics_pipeline(
    detection_results,
    dataframe,
    min_area,
    lasso_model,
    pca_model,
    classifier_model,
    logger,
    box_expansion: float,
    multiclass: bool,
):
    gt_categories, features, filenames, excluded_by_area = (
        preproc_and_features_extraction(
            detection_results,
            dataframe,
            min_area=min_area,
            logger=logger,
            box_expansion=box_expansion,
            multiclass=multiclass
        )
    )
    selected_features = features_selection(features, lasso_model, pca_model, logger)
    logger.info("Features select. Evaluating classification model.")
    classifications, metrics, conf_matrix = evaluate_classification_model(
        classifier_model,
        selected_features,
        gt_categories,
        output_dict=True,
        multiclass=multiclass,
    )
    return classifications, metrics, conf_matrix, filenames, excluded_by_area

# %% ../nbs/08_adenoma_detection_pipeline.ipynb 12
def run_pipeline(
    dataset_name: str,
    raw_image: bool = None,
    crop_to_content: bool = None,
    filter_no_annotations: bool = None,
    final_shape: list[int] = None,
    args_dict: dict = None,
    detection_score: float = None,
    detection_checkpoint: str = None,
    classification_model: str = None,
    classification_best: str = None,
    iou_threshold: float = None,
    min_area: float = 0.0,
    real_life_scenario: bool = False,
    box_expansion: float = 1,
    detection_model_folder: Path = None,
    classification_data_folder: Path = None,
    results_folder: Path = Path("results"),
    classification_type: str = "binary",
):
    multiclass = False if not classification_type == "multiclass" else True
    # get data using detection training config json
    dataloader, det_cfg, logger, res_dir = data_preparation(
        detection_model_folder=detection_model_folder,
        dataset_name=dataset_name,
        raw_image=raw_image,
        crop_to_content=crop_to_content,
        filter_no_annotations=filter_no_annotations,
        final_shape=final_shape,
        args_dict=args_dict,
        results_folder=results_folder,
        classification_type=classification_type,
    )
    dataframe = get_dataset_dataframe(det_cfg, dataset_name, multiclass)

    # load models
    logger.info("Loading detection model")
    det_model, det_config = load_detection_model(
        logger=logger,
        dataloader=dataloader,
        res_dir=res_dir,
        det_cfg=det_cfg,
        dataset_name=dataset_name,
        detection_score=detection_score,
        nms_threshold=None,
        checkpoint=detection_model_folder / f"checkpoints/{detection_checkpoint}",
    )
    # return dataloader, det_model, det_config
    logger.info("Loading classification models")
    lasso, pca, classifier = load_classification_models(
        classification_model_folder=classification_data_folder / classification_model,
        best_model_prefix=classification_best,
        logger=logger,
    )

    # machine learning pipeline
    (
        true_positives_det,
        true_negatives_det,
        false_positives_det,
        false_negatives_det,
    ) = run_lesion_detection(
        model=det_model,
        config=det_config,
        iou_threshold=iou_threshold,
        logger=logger,
    )
    detection_summary_data = {
        "true_positives_det": true_positives_det,
        "true_negatives_det": true_negatives_det,
        "false_positives_det": false_positives_det,
        "false_negatives_det": false_negatives_det,
    }
    classification_preds, metrics, conf_matrix, ordered_filenames, excluded_by_area = (
        run_radiomics_pipeline(
            detection_results=(
                {**true_positives_det, **false_positives_det}
                if real_life_scenario
                else true_positives_det
            ),
            dataframe=dataframe,
            min_area=min_area,
            lasso_model=lasso,
            pca_model=pca,
            classifier_model=classifier,
            logger=logger,
            box_expansion=box_expansion,
            multiclass=multiclass,
        )
    )

    
    classification_summary_data = {
        "classification_preds": classification_preds,
        "metrics": metrics,
        "conf_matrix": conf_matrix,
        "ordered_filenames": ordered_filenames,
        "excluded_by_area": excluded_by_area
    }
    logger.info("Pipeline finished.")
    return (
        detection_summary_data,
        classification_summary_data, 
        dataframe,
        res_dir,
    )

# %% ../nbs/08_adenoma_detection_pipeline.ipynb 13
if __name__ == "__main__":
    args = parser()
    args_dict = args_to_dict(args)
    run_pipeline(**args_dict)
