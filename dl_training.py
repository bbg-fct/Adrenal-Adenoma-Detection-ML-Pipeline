"""Using lazy config and a simple approach."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['DATA_PATH', 'parser', 'setup_logger_and_dirs', 'setup_wandb', 'get_augmentations', 'SlicesType', 'prepare_data',
           'build_evaluator', 'test_model', 'add_hooks', 'customise_model', 'prepare_default_dataloader',
           'prepare_config_before_instance', 'save_config', 'train_model']

# %% ../nbs/00_core.ipynb 4
import os
import wandb
import json
from pathlib import Path
import argparse
from detectron2.utils.logger import setup_logger
from logging import Logger
from dl_data_preparation import (
    register_datasets,
    create_dataloader_per_set,
    MyAugmentations,
)
from detectron2.config import LazyConfig, instantiate
from .ap_early_stopping_hook import APEarlyStop
from .debug_images_hook import SaveImagesHook
from detectron2.engine import hooks, default_writers, AMPTrainer, SimpleTrainer
from detectron2.utils import comm
from detectron2.evaluation import COCOEvaluator, inference_on_dataset, print_csv_format
from detectron2.checkpoint import DetectionCheckpointer
from detectron2.model_zoo import get_config, get_checkpoint_url
import warnings
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import RetinaNet
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelP6P7
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.meta_arch.retinanet import RetinaNetHead
from detectron2.config import LazyCall as L
from enum import Enum


# %% ../nbs/00_core.ipynb 6
DATA_PATH = Path(os.getenv("PHD_DATA"))

try:
    with open(Path.home() / "Adrenal-Lesion-Detection-Detectron2/config.json") as f:
        cfg = json.load(f)

    WANDB_CFG = cfg['wandb']
    GEN_CFG = cfg['general']
    PREPROC_CFG = cfg['preprocessing']
    HOOKS_CFG = cfg['hooks']
    TRAIN_CFG = cfg['training']
    ANCHOR_CFG = cfg['anchors']
    MODEL_CFG = cfg['model']
except json.JSONDecodeError:
    print("No default config json")
    WANDB_CFG = {}
    GEN_CFG = {}
    PREPROC_CFG = {}
    HOOKS_CFG = {}
    TRAIN_CFG = {}
    ANCHOR_CFG = {}
    MODEL_CFG = {}

# %% ../nbs/00_core.ipynb 8
def parser():
    parser = argparse.ArgumentParser(
        description="Starting the training process",
    )
    parser.add_argument(
        "-name",
        metavar="-n",
        type=str,
        help="name of the run",
    )
    return parser.parse_args()


def setup_logger_and_dirs(name: str, resume: bool=False) -> None:
    results_path = DATA_PATH.parent / f"results/{name}"
    if not resume:
        results_path.mkdir(parents=True, exist_ok=False)
    return setup_logger(output=(results_path / "log.log").as_posix()), results_path

# %% ../nbs/00_core.ipynb 9
def setup_wandb(logger, name: str):
    if WANDB_CFG["enabled"]:    
        wandb.init(
            project=WANDB_CFG["project"],
            entity="bbgfct",
            name=name,
            resume=None,
            config=cfg,
            sync_tensorboard=True,
        )
        logger.info("Wandb enabled.")
    else:
        logger.info("Wandb disabled.")

# %% ../nbs/00_core.ipynb 11
def get_augmentations(is_train: bool = True):
    if not is_train:
        return []
    if PREPROC_CFG["final_shape"][0] == 150:
        short_edge_range = [80, 96, 112, 128, 144, 160, 176]
        max_size = 200
    elif PREPROC_CFG["final_shape"][0] == 512:
        short_edge_range = [512, 544, 576, 608, 640]
        max_size = 800
    elif PREPROC_CFG["final_shape"][0] == 256:
        short_edge_range = [256, 288, 320, 352, 384]
        max_size = 512
    return [
        MyAugmentations.resize_shortest_edge(
            short_edge_range=short_edge_range, max_size=max_size
        ),
        MyAugmentations.random_flip(),
        MyAugmentations.random_flip(vertical=True),
        MyAugmentations.random_rotation(angle=[0,90]),
        MyAugmentations.random_brightness(mininum=0.7, maximum=1.5),
        MyAugmentations.random_contrast(mininum=0.7, maximum=1.5),
        MyAugmentations.random_elastic_transform(alpha=50.0, sigma=12.0, probability=0.5),
    ]


class SlicesType(Enum):
    WITH_CONTRAST = "with_contrast"
    NO_CONTRAST = "no_contrast"


def prepare_data(
    logger: Logger,
    custom_mapper: bool = True,
    sets: list = ["train", "val"],
    num_workers: int = 15,
    filter_no_annotations: bool = False,
    crop_to_content: bool = PREPROC_CFG.get("crop_to_content"),
    roi: bool = GEN_CFG.get("roi"),
    dataframe_path: str = GEN_CFG.get("dataframe"),
    raw_slices_names: str = GEN_CFG.get("raw_slices"),
    normalisation_type: str = PREPROC_CFG.get("normalisation"),
    enhancement_type: str = PREPROC_CFG.get("enhancement"),
    final_shape: tuple = PREPROC_CFG.get("final_shape"),
    batch_size: int = TRAIN_CFG.get("imgs_per_batch"),
    sampler_repeat_threshold: float = PREPROC_CFG.get("sampler_repeat_threshold"),
    sampler_seed: float = PREPROC_CFG.get("sampler_seed"),
    slices_type: str = GEN_CFG.get("slices_type"),
    classes: list = GEN_CFG.get("classes") 
):
    logger.info("Preparing data")

    if roi:
        slices_path = None
    else:
        if isinstance(raw_slices_names, dict):
            slices_path = {
                key: Path(os.getenv("PHD_DATA")) / raw_slices_name
                for key, raw_slices_name in raw_slices_names.items()
            }
        else:
            slices_path = Path(os.getenv("PHD_DATA")) / raw_slices_names
            
    logger.info(f"SLICES PATH {slices_path}")
    register_datasets(
        dataframe_path=Path(os.getenv("PHD_DATA")) / dataframe_path,
        classes=classes,
        logger=logger,
        slices_path=slices_path,
        roi=roi,
        sets=sets,
        slices_type=slices_type,
    )
    dataloaders = {}
    if custom_mapper:
        if crop_to_content is None:
            crop_to_content = False if roi else crop_to_content

        for set in sets:
            key = "test" if set in "val" else set
            dataloaders[key] = create_dataloader_per_set(
                set_name=set,
                logger=logger,
                crop_to_content=crop_to_content,
                normalisation_type=normalisation_type,
                enhancement_type=enhancement_type,
                final_shape=final_shape,
                filter_no_annotations=filter_no_annotations,
                num_workers=num_workers,
                imgs_per_batch=batch_size,
                augmentations=get_augmentations(is_train=set == "train"),
                sampler_repeat_thres=sampler_repeat_threshold,
                sampler_seed=sampler_seed,
            )
    return dataloaders

# %% ../nbs/00_core.ipynb 13
def build_evaluator(results_dir, dataset_name):
    return COCOEvaluator(
        dataset_name=dataset_name,
        output_dir=results_dir,
        tasks=["bbox"],
        # use_fast_impl=False,
    )

# %% ../nbs/00_core.ipynb 14
def test_model(cfg, model, logger: Logger):
    logger.info("Testing the model in the validation set.")
    if "evaluator" in cfg.dataloader:
        ret = inference_on_dataset(
            model,
            instantiate(cfg.dataloader.test),
            instantiate(cfg.dataloader.evaluator),
        )
        print_csv_format(ret)
        return ret

# %% ../nbs/00_core.ipynb 15
def add_hooks(
    lazy_config: LazyConfig,
    trainer,
    model,
    checkpointer,
    logger: Logger,
    results_path: Path,
):
    logger.info("Adding hooks.")
    hooks_list = [
        hooks.IterationTimer(),
        hooks.LRScheduler(scheduler=instantiate(lazy_config.lr_multiplier)),
        (
            hooks.PeriodicCheckpointer(checkpointer, **lazy_config.train.checkpointer)
            if comm.is_main_process()
            else None
        ),
        hooks.EvalHook(
            lazy_config.train.eval_period,
            lambda: test_model(lazy_config, model, logger=logger),
        ),
        (
            hooks.PeriodicWriter(
                default_writers(
                    lazy_config.train.output_dir, lazy_config.train.max_iter
                ),
                period=lazy_config.train.log_period,
            )
            if comm.is_main_process()
            else None
        ),
    ]
    if HOOKS_CFG["debug_images"]["enabled"]:
        logger.info("Adding train debug images hook.")
        hooks_list.append(
            SaveImagesHook(
                output_dir=results_path / "debug_images",
                trainer=trainer,
                interval=HOOKS_CFG["debug_images"]["images_interval"],
                nr_images=HOOKS_CFG["debug_images"]["nr_images"],
            )
        )
    if HOOKS_CFG["early_stop"]["enabled"]:
        logger.info("Adding early stop hook.")
        hooks_list.append(
            APEarlyStop(
                eval_period=HOOKS_CFG["eval_period"],
                logger=logger,
                patience=HOOKS_CFG["early_stop"]["patience"],
            )
        )

    trainer.register_hooks(hooks_list)
    return trainer

# %% ../nbs/00_core.ipynb 21
def customise_model(config, logger: Logger):
    resnet_depth = MODEL_CFG["resnet_depth"]
    backbone_features = MODEL_CFG["backbone_features"]

    model = L(RetinaNet)(
        backbone=L(FPN)(
            bottom_up=L(ResNet)(
                stem=L(BasicStem)(in_channels=3, out_channels=64, norm="FrozenBN"),
                stages=L(ResNet.make_default_stages)(
                    depth=resnet_depth,
                    # with depth < 50 stride_in_1x1 cannot be passed
                    stride_in_1x1=True,
                    norm="FrozenBN",
                ),
                out_features=backbone_features,
                freeze_at=MODEL_CFG["resnet_freeze_at"],
            ),
            in_features=backbone_features,
            out_channels=256,
            top_block=L(LastLevelP6P7)(
                in_channels=2048 if resnet_depth >= 50 else 512,
                out_channels="${..out_channels}",
            ),
        ),
        head=L(RetinaNetHead)(
            # Shape for each input feature map
            input_shape=[ShapeSpec(channels=256)] * 5,
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256],
            prior_prob=0.01,
            num_anchors=9,
        ),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[
                [x, x * 2 ** (1.0 / 3), x * 2 ** (2.0 / 3)]
                for x in [32, 64, 128, 256, 512]
            ],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[8, 16, 32, 64, 128],
            offset=0.0,
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        anchor_matcher=L(Matcher)(
            thresholds=[0.4, 0.5], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        num_classes=len(GEN_CFG["classes"]),
        head_in_features=MODEL_CFG["head_in_features"],
        focal_loss_alpha=MODEL_CFG["focal_loss_alpha"],
        focal_loss_gamma=MODEL_CFG["focal_loss_gamma"],
        pixel_mean=[0, 0, 0],
        pixel_std=[1, 1, 1],
        input_format="BGR",
    )
    if ANCHOR_CFG["custom"]:
        if "5000" in GEN_CFG["dataframe"]:
            model.anchor_generator.sizes = ANCHOR_CFG["data_area_cutoff"]
        else:
            model.anchor_generator.sizes = ANCHOR_CFG["data_no_cutoff"]
        model.anchor_generator.aspect_ratios = ANCHOR_CFG["aspect_ratios"]

    logger.info(
        f"Anchors -> sizes: {model.anchor_generator.sizes}, aspect ratios: {model.anchor_generator.aspect_ratios}"
    )

    return model

# %% ../nbs/00_core.ipynb 23
def prepare_default_dataloader(config):
    config.dataloader.train.dataset.names = "train"
    config.dataloader.test.dataset.names = "val"
    config.dataloader.train.total_batch_size = TRAIN_CFG["imgs_per_batch"]
    config.dataloader.test.batch_size = TRAIN_CFG["imgs_per_batch"]
    config.dataloader.train.mapper.augmentations[0].short_edge_length = [
        512,
        544,
        576,
        608,
        640,
    ]
    config.dataloader.train.mapper.augmentations[0].max_size = 800
    config.dataloader.test.mapper.augmentations = []
    config.dataloader.train.dataset.filter_empty = False
    config.dataloader.test.dataset.filter_empty = False
    return config


def prepare_config_before_instance(
    logger: Logger,
    results_path: Path,
    dataloaders: dict,
    eval_dataset: str = "val",
    model_base_config: str = MODEL_CFG.get("model_base_config"),
    trained: bool = MODEL_CFG.get("trained"),
    classes: list[str] = GEN_CFG.get("classes"),
    test_score: float = MODEL_CFG.get("test_score_thresh"),
    test: bool = False
):
    logger.info(f"Base config: {model_base_config}")

    config = get_config(model_base_config, trained=trained)

    if dataloaders:
        config.dataloader = dataloaders
    else:
        config = prepare_default_dataloader(config)

    config.dataloader.evaluator = build_evaluator(
        results_dir=results_path, dataset_name=eval_dataset
    )
    if "retinanet" in model_base_config:
        config.model = customise_model(config, logger)
    else:
        config.model.num_classes = len(classes)
        config.model.pixel_mean = [1, 1, 1]

    config.model.test_score_thresh = test_score

    if not test:
        config.train.output_dir = results_path
        config.train.eval_period = HOOKS_CFG["eval_period"]
        config.optimizer.lr = TRAIN_CFG["lr"]
    
    return config


def save_config(results_path: Path):
    with open(results_path / "config.json", "w") as f:
        json.dump(cfg, f, indent=4)


def train_model(train_name: str, resume: bool = False):
    logger, results_path = setup_logger_and_dirs(train_name, resume=resume)
    if not resume:
        save_config(results_path)

    setup_wandb(logger, train_name)

    dataloaders = prepare_data(
        logger=logger, custom_mapper=PREPROC_CFG["custom_mapper"]
    )

    config = prepare_config_before_instance(
        logger=logger, dataloaders=dataloaders, results_path=results_path
    )

    ### DO NOT CHANGE THIS ###
    model = instantiate(config.model)
    logger.info(model)
    model.to(config.train.device)
    config.optimizer.params.model = model
    optimizer = instantiate(config.optimizer)

    train_loader = instantiate(config.dataloader.train)

    if "retinanet" in MODEL_CFG["model_base_config"]:
        trainer = AMPTrainer(model, train_loader, optimizer)
    else:
        trainer = SimpleTrainer(model, train_loader, optimizer)
    checkpoints_path = results_path / "checkpoints"
    checkpoints_path.mkdir(parents=True, exist_ok=True)
    print(checkpoints_path)
    checkpointer = DetectionCheckpointer(
        model,
        checkpoints_path,
        trainer=trainer,
    )
    trainer = add_hooks(
        lazy_config=config,
        trainer=trainer,
        model=model,
        checkpointer=checkpointer,
        logger=logger,
        results_path=results_path,
    )
    checkpointer.resume_or_load(
        path=str(config.train.init_checkpoint),
        resume=resume,
    )
    if resume:
        # The checkpoint stores the training iteration that just finished, thus we start
        # at the next iteration
        start_iter = trainer.iter + 1
    else:
        start_iter = 0
    logger.info(f"Number of classes: {model.num_classes}")
    logger.info(f"Pixel Mean: {model.pixel_mean}")
    logger.info(f"Confidence Score: {model.test_score_thresh}")
    trainer.train(start_iter, config.train.max_iter)
    ###########################

# %% ../nbs/00_core.ipynb 29
if __name__ == "__main__":
    args = parser()
    warnings.simplefilter(action='ignore', category=FutureWarning)
    train_model(args.name, resume=GEN_CFG["resume"])
