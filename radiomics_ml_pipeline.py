# AUTOGENERATED! DO NOT EDIT! File to edit: ../00_pipeline.ipynb.

# %% auto 0
__all__ = ['load_mri_image', 'NormalizationType', 'min_max_normalize', 'preprocess_image', 'create_full_mask', 'add_padding',
           'extract_radiomic_features', 'pca_transform', 'ModelType', 'evaluate_model', 'exclude_slices',
           'preproc_pipeline', 'get_features', 'clean_features_df', 'get_sets', 'lasso_cv', 'xgboost_cv',
           'voting_classifier_cv', 'train_model_with_cv', 'save_model', 'selecting_classes', 'setup_logger', 'Protocol',
           'selecting_protocol', 'save_report_and_model', 'prep_dataframe']

# %% ../00_pipeline.ipynb 5
import SimpleITK as sitk
import matplotlib.pyplot as plt
import os 
from sklearn.model_selection import StratifiedGroupKFold
from enum import Enum
from pathlib import Path
import numpy as np
import pandas as pd
from sklearn.linear_model import LassoCV
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.decomposition import PCA
from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV
import json
from SimpleITK import Image
from radiomics import featureextractor
import logging
from sklearn.pipeline import Pipeline
from typing import Tuple
from datetime import datetime
import joblib
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from typing import Any, List, Union
from sklearn.metrics import f1_score, make_scorer, recall_score

# %% ../00_pipeline.ipynb 18
def load_mri_image(path: str) -> Image:
    return sitk.ReadImage(path)

# %% ../00_pipeline.ipynb 20
class NormalizationType(Enum):
    ZSCORE = 1
    MINMAX = 2
    HIST_EQ = 3
    NONE = 4

# %% ../00_pipeline.ipynb 21
def min_max_normalize(image: Image):
    array = sitk.GetArrayFromImage(image)
    min_val = np.min(array)
    max_val = np.max(array)
    normalized_array = (array - min_val) / (max_val - min_val)
    try:
        normalized_array = np.mean(normalized_array, axis=2)
    except Exception:
        pass
    normalized_image = sitk.GetImageFromArray(normalized_array)
    normalized_image.CopyInformation(image)
    return normalized_image

# %% ../00_pipeline.ipynb 23
def preprocess_image(image: Image, method: NormalizationType=NormalizationType.MINMAX):
    if method == NormalizationType.MINMAX:
        return min_max_normalize(image)
    elif method == NormalizationType.ZSCORE:
        return z_score_normalize(image)
    elif method == NormalizationType.HIST_EQ:
        return histogram_equalize(image)
    elif method == NormalizationType.NONE:
        return image
    else:
        raise ValueError(f"Normalization method not recognized. Use {list(NormalizationType.__members__.items())}.")

# %% ../00_pipeline.ipynb 38
def create_full_mask(image: Image):
    size = image.GetSize()
    mask_array = np.ones(size[::-1], dtype=np.uint32) 
    mask = sitk.GetImageFromArray(mask_array)
    return mask


def add_padding(image, padding_value=0, padding_size=1):
    padded_image = sitk.ConstantPad(
        image,
        [padding_size] * image.GetDimension(),
        [padding_size] * image.GetDimension(),
        padding_value,
    )
    return padded_image


def extract_radiomic_features(image, mask):
    extractor = featureextractor.RadiomicsFeatureExtractor()
    features = extractor.execute(add_padding(image, padding_size=2), add_padding(mask, padding_size=2))
    return features

# %% ../00_pipeline.ipynb 41
def pca_transform(X, n_components=10):
    pca = PCA(n_components=n_components)
    return pca.fit_transform(X), pca

# %% ../00_pipeline.ipynb 51
class ModelType(Enum):
    RANDOM_FOREST = 'random_forest'
    SVM = 'svm'
    LOGISTIC_REGRESSION = 'logistic_regression'
    VOTING_CLASSIFIER = 'voting_classifier'
    ADABOOST = 'adaboost'
    XGBOOST = 'xgboost'

# %% ../00_pipeline.ipynb 54
def evaluate_model(
    model,
    X_test,
    y_test,
    confusion: bool = True,
    output_dict: bool = True,
    multi: bool = False,
):
    y_pred = model.predict(X_test)
    res = None
    try:
        if multi:
            roc_auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class="ovr")
        else:
            roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, -1])
    except AttributeError:
        try:
            roc_auc = roc_auc_score(y_test, model.predict(X_test))
        except Exception:
            pass
    if output_dict:
        res = classification_report(y_test, y_pred, output_dict=True)
        res["roc_auc"] = roc_auc
    else:
        print(classification_report(y_test, y_pred, output_dict=False))
        print(roc_auc)
    if confusion:
        conf_matrix = ConfusionMatrixDisplay(
            confusion_matrix=confusion_matrix(y_test, y_pred),
            display_labels=["adenoma", "not-adenoma"],
        )
        conf_matrix.plot()
        plt.show()
    return y_pred, res

# %% ../00_pipeline.ipynb 81
def exclude_slices(dtf: pd.DataFrame, exclusion_fraction: float = 0.90):
    # PUT SETNAME AS EXCLUDE!!
    
    dtf["Series"] = dtf.Filename.apply(lambda x: x.split("_")[0])
    # DO NOT EXCLUDED NO LESION SLICES
    for patient_id in dtf.PatientID.unique():
        patient_dtf = dtf[dtf.PatientID == patient_id]
        for series in patient_dtf.Series.unique():
            series_dtf = patient_dtf[patient_dtf.Series == series]
            total_rows = len(series_dtf)
            first_percent = int(round(total_rows * exclusion_fraction / 2, 0))
            last_percent = total_rows - first_percent
            series_dtf = series_dtf.reset_index(drop=True)
            # filter_to_exclude = (series_dtf.NoLesion != "y") & (
            #     (series_dtf.index < first_percent) | (series_dtf.index >= last_percent)
            # )
            filter_to_exclude = (
                (series_dtf.index < first_percent) | (series_dtf.index >= last_percent)
            )

            series_dtf.loc[filter_to_exclude, "SetName"] = "exclude"

            dtf.loc[
                (dtf.PatientID == patient_id) & (dtf.Series == series), "SetName"
            ] = series_dtf["SetName"].values
    return dtf

# %% ../00_pipeline.ipynb 85
def preproc_pipeline(filepath: str, method: NormalizationType = NormalizationType.MINMAX):
    image = load_mri_image(filepath)
    mask = create_full_mask(image)
    try:
        preproc_image = preprocess_image(image, method=method)
    except RuntimeError:
        print(filepath)
        raise RuntimeError
    return mask, preproc_image

# %% ../00_pipeline.ipynb 91
def get_features(
    dtf: pd.DataFrame, normalize_method: NormalizationType = NormalizationType.MINMAX
):
    mask_and_preproc_images = dtf.PatchFilepathDocker.apply(
        lambda filepath: preproc_pipeline(filepath=filepath, method=normalize_method)
    ).to_list()
    features_list = [
        extract_radiomic_features(image, mask)
        for mask, image in mask_and_preproc_images
    ]
    return pd.DataFrame(features_list)

# %% ../00_pipeline.ipynb 92
def clean_features_df(features_df: pd.DataFrame):
    cols_to_drop = features_df.filter(like='Versions').columns
    features_df.drop(cols_to_drop, axis=1, inplace=True)
    cols_to_drop = features_df.filter(like='diagnostics').columns
    features_df.drop(cols_to_drop, axis=1, inplace=True)
    return features_df

# %% ../00_pipeline.ipynb 123
def get_sets(dtf_train, dtf_test, normalize_method: NormalizationType):
    X_train = clean_features_df(get_features(dtf_train, normalize_method))
    y_train = dtf_train.CategoryID
    X_test = clean_features_df(get_features(dtf_test, normalize_method))
    y_test = dtf_test.CategoryID
    return X_train, y_train, X_test, y_test

# %% ../00_pipeline.ipynb 124
def lasso_cv(X_train, X_test, y_train, cv, logging):
    logging.info("Running LassoCV...")
    alphas = np.logspace(-6, 1, 100)
    lasso = LassoCV(alphas=alphas, cv=cv, max_iter=10000)
    pipeline = Pipeline([("lasso", lasso)])
    pipeline.fit(X_train.values, y_train.values)
    selected_features = pipeline.named_steps["lasso"].coef_ != 0
    logging.info(f"Number of selected features: {selected_features.sum()}")
    lasso = pipeline.named_steps["lasso"]
    lasso.cv = None
    x_train_selected = X_train.iloc[:, selected_features]
    return (
        x_train_selected.values,
        X_test.iloc[:, selected_features].values,
        lasso,
        list(x_train_selected.columns),
    )

# %% ../00_pipeline.ipynb 125
def xgboost_cv(X, y, params: dict, cv, logging, metric=f1_score):
    logging.info("Running XGBoost CV...")
    # dtrain = DMatrix(X, label=y) 
    xgb = XGBClassifier(
        use_label_encoder=False,
        eval_metric="logloss",
        random_state=42,
        tree_method="hist",
    )
    score = make_scorer(metric, average='weighted')
    
    random_search = RandomizedSearchCV(
        estimator=xgb,
        param_distributions=params,
        n_iter=100,
        scoring=score,
        cv=cv,
        verbose=1,
        random_state=42,
    )
    # # Access cv_results_
    # cv_results = random_search.cv_results_

    # # Extract and print information
    # for key in ['mean_test_score', 'std_test_score', 'params']:
    #     print(f"{key}: {cv_results[key]}")

    random_search.fit(X, y)
    logging.info(f"Best Model Parameters: {random_search.best_params_}")
    return random_search.best_estimator_, random_search.best_params_

# %% ../00_pipeline.ipynb 126
def voting_classifier_cv(X, y, params: dict, cv, logging, metric=f1_score, voting="hard"):
    logging.info("Running Voting Classifier CV...")
    
    # Define the individual classifiers
    clf1 = XGBClassifier(
        eval_metric="logloss",
        random_state=42,
        tree_method="hist"
    )
    clf2 = RandomForestClassifier(random_state=42)
    clf3 = LogisticRegression(random_state=42, max_iter=1000)
    
    # Combine classifiers into a voting classifier
    voting_clf = VotingClassifier(
        estimators=[
            ('xgb', clf1),
            ('rf', clf2),
            ('lr', clf3)
        ],
        voting=voting,
    )
    
    score = make_scorer(metric, average='weighted')
    
    random_search = RandomizedSearchCV(
        estimator=voting_clf,
        param_distributions=params,
        n_iter=100,
        scoring=score,
        cv=cv,
        verbose=1,
        random_state=42,
    )
    
    random_search.fit(X, y)
    logging.info(f"Best Model Parameters: {random_search.best_params_}")
    return random_search.best_estimator_, random_search.best_params_ 

# %% ../00_pipeline.ipynb 127
def train_model_with_cv(
    model_type: ModelType, parameters: dict, cv_splitter, X_train, y_train, logging, metric=f1_score
):
    if model_type == ModelType.XGBOOST:
        return xgboost_cv(
            X_train,
            y_train,
            params=parameters["xgboost"],
            cv=cv_splitter,
            logging=logging,
            metric=metric,
        )
    elif model_type == ModelType.VOTING_CLASSIFIER:
        return voting_classifier_cv(
            X_train,
            y_train,
            params=parameters["voting_classifier"],
            cv=cv_splitter,
            logging=logging,
            metric=metric,
            voting=parameters["voting_type"],
        )

# %% ../00_pipeline.ipynb 128
def save_model(model, results_dir, name):
    model_filename = results_dir / f"{name}.joblib"
    joblib.dump(model, model_filename)
    return model_filename

# %% ../00_pipeline.ipynb 129
def selecting_classes(dtf: pd.DataFrame, classes: List[str]):
    if classes == ["not-adenoma", "adenoma"]:
        return dtf
    elif classes == ["adenoma", "metastasis"]:
        dtf.loc[dtf.LesionName.isin(["adenoma"]), "CategoryID"] = 0
        dtf.loc[dtf.LesionName.isin(["metastasis"]), "CategoryID"] = 1
        return dtf[dtf.LesionName.isin(["adenoma", "metastasis"])]
    elif classes == ["adenoma", "metastasis", "other"]:
        dtf.loc[dtf.LesionName.isin(["adenoma"]), "CategoryID"] = 0
        dtf.loc[dtf.LesionName.isin(["metastasis"]), "CategoryID"] = 1
        dtf.loc[~dtf.LesionName.isin(["adenoma", "metastasis"]), 'CategoryID'] = 2
        return dtf
    # adds third class no lesion
    elif classes == ["not-adenoma", "adenoma", "no-lesion"]:
        # dataframe must be special for this case!!!
        return dtf
    else:
        raise NotImplementedError

# %% ../00_pipeline.ipynb 130
def setup_logger(results_dir):
    # Create the custom logger for your application
    log = logging.getLogger(__name__)
    log.setLevel(logging.INFO)

    # File handler
    file = logging.FileHandler(f"{results_dir}/radiomics_pipeline.log")
    file.setLevel(logging.INFO)
    fileformat = logging.Formatter("%(asctime)s:%(levelname)s:%(message)s", datefmt="%H:%M:%S")
    file.setFormatter(fileformat)

    # Stream handler
    stream = logging.StreamHandler()
    stream.setLevel(logging.INFO)
    streamformat = logging.Formatter("%(asctime)s:%(levelname)s:%(message)s")
    stream.setFormatter(streamformat)

    # Add handlers to your application logger
    log.addHandler(file)
    log.addHandler(stream)

    # Configure PyRadiomics logger to use the same handlers
    radiomics_logger = logging.getLogger('radiomics')
    radiomics_logger.setLevel(logging.INFO)  # Set desired logging level
    radiomics_logger.addHandler(file)
    radiomics_logger.addHandler(stream)

    return log

# %% ../00_pipeline.ipynb 131
class Protocol(Enum):
    OPIP = "GR-['SS','SP','SK']-UNKNOWN"
    OP = "OP"
    IP = "IP"
    OTHER = "GR-['SS','SK']-UNKNOWN"
    
    def __str__(self):
        if self == Protocol.OPIP:
            return "protocol_OPIP"
        elif self == Protocol.OTHER:
            return "protocol_OTHER"
        elif self == Protocol.IP:
            return "protocol_IP"
        elif self == Protocol.OP:
            return "protocol_OP"

def selecting_protocol(dtf, protocol: Union[str, None] = None) -> pd.DataFrame:
    if protocol is None:
        return dtf
    elif protocol in [Protocol.IP.value, Protocol.OP.value]:
        dtf = dtf[dtf.Sequence == Protocol.OPIP.value] 
        if protocol == Protocol.IP.value:
            in_phase_df = dtf.iloc[1::2]
            return in_phase_df.reset_index()
        else:
            out_of_phase_df = dtf.iloc[::2]
            return out_of_phase_df.reset_index()
    else:
        return dtf[dtf.Sequence == protocol]


# %% ../00_pipeline.ipynb 132
def save_report_and_model(
    report: Tuple[np.ndarray, dict],
    exclusion_fraction: float,
    n_features: int,
    results_dir: str,
    best_params: dict,
    n_folds: int,
    model_type: str,
    model: Any,
    date: str,
    lasso: LassoCV,
    pca: PCA,
    classes: List[str],
    protocol: Protocol,
    selected_features: list,
    remove_no_lesion: bool = None,
):
    y_pred, report = report
    report["best_params"] = best_params
    report["selected_features"] = selected_features
    report["predictions"] = y_pred.tolist()

    classes_str = "_vs_".join(classes)
    folder = f"{results_dir}/{date}_{classes_str}_{n_folds}_{model_type}_{protocol}" 
    if remove_no_lesion is not None:
        folder = folder + f"_remove_no_lesions-{remove_no_lesion}"
    folder = Path(folder)
    folder.mkdir(parents=True, exist_ok=True)
    name = f"feat_{n_features}-exclusion_{int(exclusion_fraction*100)}"
    report_filename = folder / f"{name}-evaluation.json" 
    open(
        report_filename,
        "w",
    ).write(json.dumps(report))
    save_model(model, folder, f"{name}-model")
    save_model(lasso, folder, f"{name}-lasso")
    save_model(pca, folder, f"{name}-pca")
    return report_filename.parent

# %% ../00_pipeline.ipynb 134
def prep_dataframe(dtf, classes, remove_no_lesion=False):
    try:
        if remove_no_lesion:
            dtf = dtf[dtf.NoLesion != "y"]
    except AttributeError:
        logging.warning("NoLesion data not available")
        pass
    dtf["Series"] = dtf.Filename.apply(lambda x: x.split("_")[0])
    dtf = selecting_classes(dtf, classes=classes)
    logging.info(f"Dataframe length after preparation: {len(dtf)}")
    return dtf

# %% ../00_pipeline.ipynb 135
if __name__ == "__main__":
    parameters = {
        "voting_type": "hard",
        "exclusion_fraction": [0.3, 0.5, 0.8],
        "n_features": [5, 10, 15],
        "xgboost": {
            "n_estimators": [50, 100, 150],
            "learning_rate": [0.01, 0.1, 0.2],
            "max_depth": [3, 4, 5],
            "subsample": [0.8, 0.9, 1.0],
            "colsample_bytree": [0.8, 0.9, 1.0],
            "reg_lambda": [0.5, 1.0, 1.5],
            "reg_alpha": [0.0, 0.1, 0.2],
        },
        "voting_classifier": {
            
            "xgb__n_estimators": [50, 100, 200],
            "xgb__learning_rate": [0.01, 0.1, 0.2],
            "rf__n_estimators": [50, 100, 200],
            "lr__C": [0.1, 1.0, 10.0],
        },
    }

    DATE = datetime.now().strftime("%Y-%m-%d-%H-%M")
    METRIC = recall_score
    CV_SPLITS = [6, 10]
    REMOVE_NO_LESIONS = True
    CLASSES = ["not-adenoma", "adenoma"]  # , "no-lesion"]
    PROTOCOLS = [None]  # [Protocol.OP, Protocol.IP]

    MODELS = [ModelType.VOTING_CLASSIFIER]  # , ModelType.XGBOOST]

    RESULTS_DIR = f"/home/bbg/results/{'_vs_'.join(CLASSES)}_no-lesion-slices-{not REMOVE_NO_LESIONS}-hard_voting"
    Path(RESULTS_DIR).mkdir(exist_ok=True)

    logging = setup_logger(RESULTS_DIR)
    logging.info("Getting started...")
    logging.info(f"Removing no lesion slices active?: {REMOVE_NO_LESIONS}")
    logging.info(f"Selected classes: {CLASSES}")

    ROOT_DIR_DATA = Path(os.getenv("PHD_DATA", "data"))
    # SLICES_DTF_PATH = (
    #     ROOT_DIR_DATA
    #     / "data_sheets/garcia_orta_adrenal/lesion_training/lesions_data-ready_for_training_with_no_lesions_IMPROVED.feather"
    # )
    SLICES_DTF_PATH = (
        ROOT_DIR_DATA
        / "data_sheets/garcia_orta_adrenal/lesion_training/lesion_classification_DL_split.csv"
    )

    # original_dtf = pd.read_feather(SLICES_DTF_PATH)
    original_dtf = pd.read_csv(SLICES_DTF_PATH)

    logging.info(f"Original Dataframe loaded from {SLICES_DTF_PATH}")
    logging.info(f"Original length {len(original_dtf)}")

    dtf = prep_dataframe(original_dtf, CLASSES, REMOVE_NO_LESIONS)

    for protocol in PROTOCOLS:
        logging.info(f"Selecting protocol {protocol}")
        protocol_dtf = selecting_protocol(
            dtf, protocol=protocol.value if protocol is not None else None
        )
        for model in MODELS:
            logging.info(f"Training {model.value} model...")

            for n_fold in CV_SPLITS:
                cv_splitter = StratifiedGroupKFold(n_splits=n_fold, shuffle=True)

                for exclusion_fraction in parameters["exclusion_fraction"]:
                    logging.info(f"Excluding {exclusion_fraction} slices")

                    dataframe_filtered = exclude_slices(
                        protocol_dtf.copy(), exclusion_fraction
                    ).reset_index(drop=True)
                    dtf_train = dataframe_filtered[
                        dataframe_filtered.SetName.isin(["train", "val"])
                    ]
                    logging.info(
                        f"Data for the pipeline - {len(dataframe_filtered)} slices."
                    )

                    logging.info(f"Training set size: {len(dtf_train)}")

                    logging.info(
                        f"Adenoma class size: {len(dtf_train[dtf_train.CategoryID == 1])}"
                    )
                    logging.info(
                        f"Not-adenoma class size: {len(dtf_train[dtf_train.CategoryID == 0])}"
                    )

                    dtf_test = dataframe_filtered[
                        dataframe_filtered.SetName.isin(["test"])
                    ]

                    logging.info(f"Test set size: {len(dtf_test)}")
                    logging.info(
                        f"Adenoma class size: {len(dtf_test[dtf_test.CategoryID == 1])}"
                    )
                    logging.info(
                        f"Not-adenoma class size: {len(dtf_test[dtf_test.CategoryID == 0])}"
                    )
                    X_train, y_train, X_test, y_test = get_sets(
                        dtf_train, dtf_test, normalize_method=NormalizationType.MINMAX
                    )
                    assert len(X_train) == len(y_train) and len(X_test) == len(y_test)

                    (
                        X_train_selected,
                        X_test_selected,
                        lasso_model,
                        selected_features,
                    ) = lasso_cv(
                        X_train,
                        X_test,
                        y_train,
                        cv=cv_splitter.split(
                            X_train, y_train, groups=dtf_train.PatientID
                        ),
                        logging=logging,
                    )
                    y_train = y_train.values
                    y_test = y_test.values
                    assert (
                        isinstance(X_train_selected, np.ndarray)
                        and isinstance(y_train, np.ndarray)
                        and isinstance(X_test_selected, np.ndarray)
                        and isinstance(y_test, np.ndarray)
                    )
                    for n_components in parameters["n_features"]:
                        assert n_components <= min(X_train.shape[0], X_train.shape[1])
                        logging.info(f"Reduction to {n_components} features...")
                        try:
                            X_train_reduced, pca_train = pca_transform(
                                X_train_selected, n_components=n_components
                            )
                            X_test_reduced, pca_test = pca_transform(
                                X_test_selected, n_components=n_components
                            )
                        except ValueError:
                            logging.error(
                                "Number of components exceeds the number of features."
                            )
                            continue
                        best_model, best_params = train_model_with_cv(
                            model_type=model,
                            parameters=parameters,
                            X_train=X_train_reduced,
                            y_train=y_train,
                            cv_splitter=cv_splitter.split(
                                X_train, y_train, groups=dtf_train.PatientID
                            ),
                            logging=logging,
                            metric=METRIC,
                        )
                        best_model.fit(X_train_reduced, y_train)
                        report = evaluate_model(
                            best_model,
                            X_test_reduced,
                            y_test,
                            confusion=False,
                            output_dict=True,
                        )
                        results_dir = save_report_and_model(
                            report=report,
                            exclusion_fraction=exclusion_fraction,
                            n_features=n_components,
                            results_dir=RESULTS_DIR,
                            best_params=best_params,
                            n_folds=n_fold,
                            model_type=model.value,
                            model=best_model,
                            lasso=lasso_model,
                            pca=pca_test,
                            date=DATE,
                            classes=CLASSES,
                            protocol=protocol,
                            remove_no_lesion=REMOVE_NO_LESIONS,
                            selected_features=selected_features,
                        )
                        logging.info(f"Results saved in {results_dir}")
    logging.info("Process completed.")
